<html><head>
    <script src="chrome-extension://khpkpbbcccdmmclmpigdgddabeilkdpd/dapp-api.js" type="module"></script><link rel="stylesheet" href="main.css">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
  </head>
<body style="background-color:#f5f5f5;">
  <h1>Zhili Feng</h1>
  <div class="intro">
    <img src="photo.jpg" width="200" height="200" style="float:left; padding-right:10px">
Hello! I am a PhD student in the Machine Learning Department at Carnegie Mellon University. I am lucky to be advised by Prof. <a href="https://zicokolter.com/">Zico Kolter</a>. <br><br>
I am interested in designing algorithms that guide foundation models to satisfy certain downstream requirement -- e.g. robustness, privacy, or safety.
I am also generally interested in information theory and compression and their applications in understanding machine learning. <i>Link to <a href="./zhili_cv.pdf">CV</a></i>
<br>
<br>
</div>
<div class="contact">
<h3>Email</h3>
zhilif at andrew dot cmu dot edu <br><br>

<!-- add google scholar, linkedin, and twitter here-->
<!-- <div class="contact">
<h3>Links</h3> -->
<a href="https://scholar.google.com/citations?user=_lnL4aQAAAAJ&hl=en&authuser=1">Google Scholar</a> &nbsp;
<a href="https://x.com/zhilifeng">Twitter</a> &nbsp;
<a href="https://www.linkedin.com/in/zhili-feng-47768a55/">LinkedIn</a>

</div>
<br>
<br>
<div style="border-bottom: 2px solid #333;"></div>

<div class="research">
<h3>Selected Publications</h3>
<ul>
  <li> <b> Rethinking LLM Memorization through the Lens of Adversarial Compression  </b><br>
    with Avi Schwarzchild, Pratyush Maini, Zachary C. Lipton, and Zico Kolter<br>
    <a href="url">https://arxiv.org/abs/2404.15146</a>
  </li>
  <li> <b> TOFU: A Task of Fictitious Unlearning for LLMs  </b><br>
    with Pratyush Maini, Avi Schwarzchild, Zachary C. Lipton, and Zico Kolter<br>
    COLM2024
  </li>
  <li> <b> Sequence-level large language model training with noise contrastive preference optimization  </b><br>
    <i>Link to <a href="./ebm_llm.pdf">Work in progress</a></i>
  </li>
  <li> <b>	Text descriptions are compressive and invariant representations for visual learning  </b><br>
    with Anna Bair, Zico Kolter<br>
    TMLR2024
    <!-- <a href="url">https://arxiv.org/abs/2307.04317</a> -->
  </li>

  <li> <b>Monotone deep Boltzmann machines </b><br>
    with Ezra Winston, Zico Kolter<br>
    TMLR2023
  </li>

  <li> <b>On the neural tangent kernel of equilibrium models </b><br>
    with Zico Kolter<br>
    <a href="url">https://arxiv.org/pdf/2310.14062.pdf</a>
  </li>

  <li> <b>Provable adaptation across multiway domains via representation learning </b><br>
    with Shaobo Han, and Simon S. Du<br>
    ICLR2021
  </li>
  
  <li> <b> Non-psd matrix sketching with applications to regression and optimization </b><br>
    with Fred Roosta, and David P. Woodruff<br>
    UAI2021
  </li>
  
  <li> <b>Learning-augmented k-means clustering</b><br>
    with Jon Ergun, Sandeep Silwal, David P. Woodruff, and Samson Zhou<br>
    ICLR 2021
  </li>

  <li> <b>Dimensionality reduction for the sum-of-distances metric</b><br>
      with Praneeth Kacham, and David Woodruff<br>
      ICML 2021
  </li>

  <li> <b>Does data augmentation lead to positive margin?</b><br>
    with Shashank Rajput, Zachary B. Charles, Po-Ling Loh, and Dimitris S. Papailiopoulos<br>
    ICML 2019
  </li>
  
  <li> <b>Online learning with graph-structured feedback against adaptive adversaries</b><br>
    with Po-Ling Loh<br>
    ISIT 2018
  </li>

  <li> <b>Joint reasoning for temporal and causal relations</b><br>
    with Qiang Ning, and Dan Roth<br>
    ACL 2018
  </li>

  <li> <b>A structured learning approach to temporal relation extraction</b><br>
    with Qiang Ning, and Dan Roth<br>
    EMNLP 2017
  </li>
  
  
</ul>
</div>


</body></html>